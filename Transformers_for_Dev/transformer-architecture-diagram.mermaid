%%{init: {'theme': 'dark', 'themeVariables': { 'primaryColor': '#2C3E50', 'primaryTextColor': '#FFFFFF', 'lineColor': '#FFFFFF', 'fontSize': '14px'}}}%%
graph TD
    A[("Input Embeddings")]:::input --> B[("+ Positional Encoding")]:::input
    B --> C{"Multi-Head Attention"}:::attention
    C --> D["Add & Norm"]:::norm
    D --> E["Feed Forward"]:::ffn
    E --> F["Add & Norm"]:::norm
    F --> G[("Output")]:::output
    
    subgraph "Multi-Head Attention"
        H["Q: Query"]:::attention
        I["K: Key"]:::attention
        J["V: Value"]:::attention
        K["MatMul & Scale"]:::attention
        L["Softmax"]:::attention
        M["MatMul"]:::attention
        H & I --> K
        K --> L
        L & J --> M
    end
    
    subgraph "Feed Forward"
        N["Linear"]:::ffn
        O["ReLU"]:::ffn
        P["Linear"]:::ffn
        N --> O --> P
    end
    
    C -. "Residual" .-> D
    E -. "Residual" .-> F
    
    classDef input fill:#3498DB,stroke:#FFFFFF,stroke-width:2px;
    classDef attention fill:#27AE60,stroke:#FFFFFF,stroke-width:2px;
    classDef norm fill:#F39C12,stroke:#FFFFFF,stroke-width:2px;
    classDef ffn fill:#8E44AD,stroke:#FFFFFF,stroke-width:2px;
    classDef output fill:#E74C3C,stroke:#FFFFFF,stroke-width:2px;
